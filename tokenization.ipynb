{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenization - Used for preprocessing data\n",
    "\n",
    "tokens - to convert a para into sentence(senetnce tokenizer), \n",
    "\n",
    "sentence to words(words tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"\n",
    "I am learning Natural Language Processing.\n",
    "Using NLTK Library.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paragraph ---> sentence\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nI am learning Natural Language Processing.', 'Using NLTK Library.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paragraph ---> words\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " '.',\n",
       " 'Using',\n",
       " 'NLTK',\n",
       " 'Library',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " '.',\n",
       " 'Using',\n",
       " 'NLTK',\n",
       " 'Library',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing.',\n",
       " 'Using',\n",
       " 'NLTK',\n",
       " 'Library',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokenizer</th>\n",
       "      <th>Purpose</th>\n",
       "      <th>Key Behavior</th>\n",
       "      <th>Example</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Word Tokenizer</td>\n",
       "      <td>Breaks text into words</td>\n",
       "      <td>Splits text based on whitespace and punctuation</td>\n",
       "      <td>This is a sample sentence. -&gt; ['This', 'is', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence Tokenizer</td>\n",
       "      <td>Divides text into sentences</td>\n",
       "      <td>Identifies sentence boundaries based on punctu...</td>\n",
       "      <td>This is the first sentence. This is the second...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tweet Tokenizer</td>\n",
       "      <td>Tokenizes tweets</td>\n",
       "      <td>Handles Twitter-specific features like hashtag...</td>\n",
       "      <td>RT @user: This is a #great tweet! :) -&gt; ['RT',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Punkt Sentence Tokenizer</td>\n",
       "      <td>Sentence tokenization</td>\n",
       "      <td>Employs machine learning for accurate sentence...</td>\n",
       "      <td>This is a complex sentence with multiple claus...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Tokenizer                      Purpose  \\\n",
       "0            Word Tokenizer       Breaks text into words   \n",
       "1        Sentence Tokenizer  Divides text into sentences   \n",
       "2           Tweet Tokenizer             Tokenizes tweets   \n",
       "3  Punkt Sentence Tokenizer        Sentence tokenization   \n",
       "\n",
       "                                        Key Behavior  \\\n",
       "0    Splits text based on whitespace and punctuation   \n",
       "1  Identifies sentence boundaries based on punctu...   \n",
       "2  Handles Twitter-specific features like hashtag...   \n",
       "3  Employs machine learning for accurate sentence...   \n",
       "\n",
       "                                             Example  \n",
       "0  This is a sample sentence. -> ['This', 'is', '...  \n",
       "1  This is the first sentence. This is the second...  \n",
       "2  RT @user: This is a #great tweet! :) -> ['RT',...  \n",
       "3  This is a complex sentence with multiple claus...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = pd.read_csv('nltk token.csv')\n",
    "tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
